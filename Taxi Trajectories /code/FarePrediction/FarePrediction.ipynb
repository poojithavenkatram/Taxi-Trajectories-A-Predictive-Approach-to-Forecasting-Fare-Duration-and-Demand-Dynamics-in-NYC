{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow\n",
        "!pip install fastparquet\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install xgboost\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "rfE9i5ZGhmqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4961932f-b381-4900-f05f-6a633b7a3cef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow\n",
            "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n",
            "Installing collected packages: pyarrow\n",
            "Successfully installed pyarrow-16.0.0\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.25.2)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2024.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.8.3 fastparquet-2024.2.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-2.0.3\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, path_prefix):\n",
        "        self.path_prefix = path_prefix\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and return a subset of the dataset to reduce memory usage.\"\"\"\n",
        "        df = pd.read_parquet(self.path_prefix + '/full_df.parquet')\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        df = df.copy()\n",
        "        df = self.encode_and_extend_df(df)\n",
        "        df = self.encode_days_of_week(df)\n",
        "        df = self.drop_duplicates(df)\n",
        "        df = self.distance_calculation(df)\n",
        "        df = self.drop_columns(df)\n",
        "        df = self.outlier_correction(df)\n",
        "        df = self.passenger_count_filter(df)\n",
        "        df.fillna(0, inplace=True)  # Fill missing values to avoid NaN issues\n",
        "        self.corr_matrix(df)\n",
        "        return df\n",
        "\n",
        "    def encode_and_extend_df(self, df):\n",
        "        \"\"\"Encode categorical features.\"\"\"\n",
        "        time_of_day_mapping = {'Morning': 0, 'Afternoon': 1, 'Evening': 2, 'Late night': 3}\n",
        "        if 'pickup_timeofday' in df.columns and 'dropoff_timeofday' in df.columns:\n",
        "            df['pickup_timeofday_encoded'] = df['pickup_timeofday'].map(time_of_day_mapping)\n",
        "            df['dropoff_timeofday_encoded'] = df['dropoff_timeofday'].map(time_of_day_mapping)\n",
        "        else:\n",
        "            raise ValueError(\"DataFrame is missing necessary time of day columns.\")\n",
        "        return df\n",
        "\n",
        "    def encode_days_of_week(self, df):\n",
        "        \"\"\"Encode categorical features.\"\"\"\n",
        "        day_mapping = {\n",
        "            'Monday': 0,\n",
        "            'Tuesday': 1,\n",
        "            'Wednesday': 2,\n",
        "            'Thursday': 3,\n",
        "            'Friday': 4,\n",
        "            'Saturday': 5,\n",
        "            'Sunday': 6\n",
        "        }\n",
        "\n",
        "        # Check if the necessary columns are in the DataFrame\n",
        "        if 'pickup_day' in df.columns and 'dropoff_day' in df.columns:\n",
        "            # Apply the mapping to the DataFrame\n",
        "            df['pickup_day_encoded'] = df['pickup_day'].map(day_mapping)\n",
        "            df['dropoff_day_encoded'] = df['dropoff_day'].map(day_mapping)\n",
        "        else:\n",
        "            raise ValueError(\"DataFrame does not contain the 'pickup_day' or 'dropoff_day' columns.\")\n",
        "\n",
        "        # Return the DataFrame with the new columns added\n",
        "        return df\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        # Earth radius in miles\n",
        "        R = 3959.0\n",
        "\n",
        "        # Convert latitude and longitude to radians\n",
        "        lat1_rad = radians(lat1)\n",
        "        lon1_rad = radians(lon1)\n",
        "        lat2_rad = radians(lat2)\n",
        "        lon2_rad = radians(lon2)\n",
        "\n",
        "        # Haversine formula\n",
        "        dlon = lon2_rad - lon1_rad\n",
        "        dlat = lat2_rad - lat1_rad\n",
        "        a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
        "        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "        distance = R * c\n",
        "\n",
        "        return distance\n",
        "\n",
        "    def corr_matrix(self, df):\n",
        "        corr_matrix = df.corr()\n",
        "        corr_matrix['fare_amount'].sort_values(ascending=False)\n",
        "\n",
        "    def distance_calculation(self, df):\n",
        "        df['orthodromic_distance'] = df.apply(lambda row: self.haversine_distance(\n",
        "            row['pickup_latitude'], row['pickup_longitude'],\n",
        "            row['dropoff_latitude'], row['dropoff_longitude']\n",
        "        ), axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def drop_duplicates(self, df):\n",
        "        \"\"\"Drop duplicate rows.\"\"\"\n",
        "        df.drop_duplicates(inplace=True)\n",
        "        return df\n",
        "\n",
        "    def drop_columns(self, df):\n",
        "        \"\"\"Drop unnecessary columns.\"\"\"\n",
        "        columns_to_drop = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'store_and_fwd_flag',\n",
        "                           'pickup_timeofday', 'dropoff_timeofday', 'pickup_day', 'dropoff_day',\n",
        "                           'pickup_datetime', 'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude',\n",
        "                           'pickup_longitude', 'pickup_latitude', 'orthodromic_distance']\n",
        "        df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "        return df\n",
        "\n",
        "    def outlier_correction(self,df):\n",
        "        df = df[(df['distance_miles'] > 1.0) & (df['distance_miles'] < 10.0)]\n",
        "        df = df[(df['fare_amount'] > 0.0) & (df['fare_amount'] < 50.0)]\n",
        "        return df\n",
        "\n",
        "    def passenger_count_filter(self, df):\n",
        "        #We will remove the other passenger counts since they are a minority and only keep records for count=1\n",
        "        sns.countplot(x=df['passenger_count'])\n",
        "        df = df[df['passenger_count'] == 1]\n",
        "        df = df.drop(['passenger_count'], axis=1)\n",
        "        return df\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def split_data(self, target_col='fare_amount', test_size=0.2, val_size=0.25):\n",
        "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
        "        train_val, test = train_test_split(self.df, test_size=test_size, random_state=42)\n",
        "        train, val = train_test_split(train_val, test_size=val_size, random_state=42)\n",
        "        return train, val, test\n",
        "\n",
        "    def pca_standardize(self, X_train, X_val, X_test, n_components=5):\n",
        "        \"\"\"Perform PCA and standardization on the data.\"\"\"\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "        X_val_pca = pca.transform(X_val_scaled)\n",
        "        X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "        return X_train_pca, X_val_pca, X_test_pca\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train the model with provided training data.\"\"\"\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions with the model.\"\"\"\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def evaluate(self, y_true, y_pred):\n",
        "        \"\"\"Evaluate model performance using RMSE and MAE.\"\"\"\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred)), mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train the model and evaluate performance on the validation set.\"\"\"\n",
        "        self.train(X_train, y_train)\n",
        "        y_val_pred = self.predict(X_val)\n",
        "        val_rmse, val_mae = self.evaluate(y_val, y_val_pred)\n",
        "        return val_rmse, val_mae\n",
        "\n",
        "    def test_model(self, X_test, y_test):\n",
        "        \"\"\"Test the model and return the RMSE on the test set.\"\"\"\n",
        "        y_test_pred = self.predict(X_test)\n",
        "        test_rmse, test_mae = self.evaluate(y_test, y_test_pred)\n",
        "        return test_rmse, test_mae\n",
        "\n",
        "class LinearRegressionModel(BaseModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(LinearRegression())\n",
        "\n",
        "class XGBoostModel(BaseModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(xgb.XGBRegressor(n_estimators=200,max_depth=5,learning_rate=0.1,objective='reg:squarederror',n_jobs=-1,random_state=42))\n",
        "\n",
        "    def grid_search(self, X_train, y_train):\n",
        "        \"\"\"Perform Grid Search for hyperparameter tuning.\"\"\"\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5],\n",
        "            'learning_rate': [0.1, 0.01],\n",
        "            'objective': ['reg:squarederror']\n",
        "        }\n",
        "        scoring = 'neg_mean_absolute_error'\n",
        "        n_splits = 5\n",
        "        kf = KFold(n_splits=n_splits)\n",
        "\n",
        "        grid_search = GridSearchCV(self.model, param_grid, scoring=scoring, cv=kf, verbose=10)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        self.best_params = grid_search.best_params_\n",
        "        print(\"Best hyperparameters:\", self.best_params)\n",
        "\n",
        "        # Update the model with the best parameters\n",
        "        self.model = xgb.XGBRegressor(**self.best_params, n_jobs=-1, random_state=42)\n",
        "\n",
        "class RandomForestModel(BaseModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(RandomForestRegressor(n_estimators=100, verbose=2))\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    data_loader = DataLoader('/app')\n",
        "    full_df = data_loader.load_data()\n",
        "    processed_df = data_loader.preprocess_data(full_df)\n",
        "    print(processed_df.head())\n",
        "    data_processor = DataProcessor(processed_df)\n",
        "\n",
        "    # Split data\n",
        "    train_df, val_df, test_df = data_processor.split_data(target_col='fare_amount')\n",
        "\n",
        "    # Extract features and target\n",
        "    target_col = 'fare_amount'\n",
        "    features = ['distance_miles', 'tolls_amount', 'tip_amount',\n",
        "                           'pickup_timeofday_encoded', 'dropoff_timeofday_encoded',\n",
        "                           'trip_distance', 'airport_fee', 'total_amount']\n",
        "\n",
        "    X_train, y_train = train_df[features], train_df[target_col]\n",
        "    X_val, y_val = val_df[features], val_df[target_col]\n",
        "    X_test, y_test = test_df[features], test_df[target_col]\n",
        "\n",
        "    # Standardize and perform PCA\n",
        "    X_train_pca, X_val_pca, X_test_pca = data_processor.pca_standardize(X_train, X_val, X_test)\n",
        "\n",
        "    # Linear Regression Model\n",
        "    linear_regression_model = LinearRegressionModel()\n",
        "\n",
        "    # Train and evaluate Linear Regression model\n",
        "    val_rmse_lr, val_mae_lr = linear_regression_model.train_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    print(f'Linear Regression Validation RMSE: {val_rmse_lr}')\n",
        "    print(f'Linear Regression Validation MAE: {val_mae_lr}')\n",
        "\n",
        "    # Test model - Linear Regression\n",
        "    test_rmse_lr, test_mae_lr = linear_regression_model.test_model(X_test, y_test)\n",
        "\n",
        "    print(f'Linear Regression Test RMSE: {test_rmse_lr}')\n",
        "    print(f'Linear Regression Test MAE: {test_mae_lr}')\n",
        "\n",
        "    #XGBoost Model with PCA\n",
        "    xgboost_model = XGBoostModel()\n",
        "\n",
        "    # Train and evaluate XGBoost model\n",
        "    val_rmse_xgb, val_mae_xgb = xgboost_model.train_model(X_train_pca, y_train, X_val_pca, y_val)\n",
        "\n",
        "    print(f'XGBoost Validation RMSE: {val_rmse_xgb}')\n",
        "    print(f'XGBoost Validation MAE: {val_mae_xgb}')\n",
        "\n",
        "    # Test model - XGBoost\n",
        "    test_rmse_xgb, test_mae_xgb = xgboost_model.test_model(X_test_pca, y_test)\n",
        "\n",
        "    print(f'XGBoost Test RMSE: {test_rmse_xgb}')\n",
        "    print(f'XGBoost Test MAE: {test_mae_xgb}')\n",
        "\n",
        "    #XGBoost Model with PCA and grid search hyperparameter tuning\n",
        "    xgboost_model_gs = XGBoostModel()\n",
        "\n",
        "    # Perform grid search\n",
        "    xgboost_model_gs.grid_search(X_train_pca, y_train)\n",
        "\n",
        "    # Train and evaluate XGBoost model after GridSearch\n",
        "    val_rmse_xgb_gs, val_mae_xgb_gs = xgboost_model_gs.train_model(X_train_pca, y_train, X_val_pca, y_val)\n",
        "\n",
        "    print(f'XGBoost after Grid Search Validation RMSE: {val_rmse_xgb_gs}')\n",
        "    print(f'XGBoost after Grid Search Validation MAE: {val_mae_xgb_gs}')\n",
        "\n",
        "    # Test model - XGBoost after GridSearch\n",
        "    test_rmse_xgb_gs, test_mae_xgb_gs = xgboost_model_gs.test_model(X_test_pca, y_test)\n",
        "\n",
        "    print(f'XGBoost after Grid Search Test RMSE: {test_rmse_xgb_gs}')\n",
        "    print(f'XGBoost after Grid Search Test MAE: {test_mae_xgb_gs}')\n",
        "\n",
        "    # Random Forest after PCA\n",
        "\n",
        "    rf_model = RandomForestModel()\n",
        "\n",
        "    # Train and evaluate XGBoost model\n",
        "    val_rmse_rf, val_mae_rf = rf_model.train_model(X_train_pca, y_train, X_val_pca, y_val)\n",
        "\n",
        "    print(f'Random Forest Validation RMSE: {val_rmse_rf}')\n",
        "    print(f'Random Forest Validation MAE: {val_mae_rf}')\n",
        "\n",
        "    # Test model - Random Forest\n",
        "    test_rmse_rf, test_mae_rf = rf_model.test_model(X_test_pca, y_test)\n",
        "\n",
        "    print(f'Random Forest Test RMSE: {test_rmse_rf}')\n",
        "    print(f'Random Forest Test MAE: {test_mae_rf}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rRd72hVfboCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fa5c0d0-102c-4eda-d474-264a25b769fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   VendorID  trip_distance  PULocationID  DOLocationID  payment_type  \\\n",
            "0         2           0.00           238            42             2   \n",
            "3         1           2.90           140            43             1   \n",
            "4         2           1.23            79           137             1   \n",
            "5         1           1.20           162           137             1   \n",
            "6         1           1.80           170            48             1   \n",
            "\n",
            "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ...  pickup_hour  \\\n",
            "0          8.6    1.0      0.5        0.00           0.0  ...            0   \n",
            "3         15.6    3.5      0.5        4.10           0.0  ...            0   \n",
            "4          7.2    1.0      0.5        2.44           0.0  ...            0   \n",
            "5         10.0    3.5      0.5        3.00           0.0  ...            0   \n",
            "6         12.1    3.5      0.5        3.40           0.0  ...            0   \n",
            "\n",
            "   dropoff_hour  pickup_month  dropoff_month  distance_miles  trip_duration  \\\n",
            "0             0             3              3        2.491304      10.000000   \n",
            "3             1             3              3        1.302484      11.466667   \n",
            "4             0             3              3        1.013622       3.033333   \n",
            "5             0             3              3        1.142584       8.416667   \n",
            "6             0             3              3        1.164417       9.783333   \n",
            "\n",
            "   pickup_timeofday_encoded  dropoff_timeofday_encoded  pickup_day_encoded  \\\n",
            "0                         3                          3                   2   \n",
            "3                         3                          3                   2   \n",
            "4                         3                          3                   2   \n",
            "5                         3                          3                   2   \n",
            "6                         3                          3                   2   \n",
            "\n",
            "   dropoff_day_encoded  \n",
            "0                    2  \n",
            "3                    2  \n",
            "4                    2  \n",
            "5                    2  \n",
            "6                    2  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "Linear Regression Validation RMSE: 1.2073461603164117\n",
            "Linear Regression Validation MAE: 0.8335794373415482\n",
            "Linear Regression Test RMSE: 1.2065880362177486\n",
            "Linear Regression Test MAE: 0.8329508270985525\n",
            "XGBoost Validation RMSE: 2.0326543819563065\n",
            "XGBoost Validation MAE: 1.3722901025951728\n",
            "XGBoost Test RMSE: 2.03559697720603\n",
            "XGBoost Test MAE: 1.3726895795256981\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV 1/5; 1/8] START learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 1/5; 1/8] END learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-1.762 total time=  13.5s\n",
            "[CV 2/5; 1/8] START learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 2/5; 1/8] END learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-1.770 total time=  14.2s\n",
            "[CV 3/5; 1/8] START learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 3/5; 1/8] END learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-1.759 total time=  13.7s\n",
            "[CV 4/5; 1/8] START learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 4/5; 1/8] END learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-1.763 total time=  13.4s\n",
            "[CV 5/5; 1/8] START learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 5/5; 1/8] END learning_rate=0.1, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-1.763 total time=  13.4s\n",
            "[CV 1/5; 2/8] START learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 1/5; 2/8] END learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-1.634 total time=  21.7s\n",
            "[CV 2/5; 2/8] START learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 2/5; 2/8] END learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-1.639 total time=  25.2s\n",
            "[CV 3/5; 2/8] START learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 3/5; 2/8] END learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-1.632 total time=  21.1s\n",
            "[CV 4/5; 2/8] START learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 4/5; 2/8] END learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-1.637 total time=  23.9s\n",
            "[CV 5/5; 2/8] START learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 5/5; 2/8] END learning_rate=0.1, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-1.633 total time=  26.3s\n",
            "[CV 1/5; 3/8] START learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 1/5; 3/8] END learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-1.498 total time=  16.2s\n",
            "[CV 2/5; 3/8] START learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 2/5; 3/8] END learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-1.491 total time=  16.1s\n",
            "[CV 3/5; 3/8] START learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 3/5; 3/8] END learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-1.493 total time=  17.7s\n",
            "[CV 4/5; 3/8] START learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 4/5; 3/8] END learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-1.493 total time=  16.2s\n",
            "[CV 5/5; 3/8] START learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 5/5; 3/8] END learning_rate=0.1, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-1.487 total time=  16.7s\n",
            "[CV 1/5; 4/8] START learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 1/5; 4/8] END learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-1.376 total time=  30.2s\n",
            "[CV 2/5; 4/8] START learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 2/5; 4/8] END learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-1.366 total time=  30.1s\n",
            "[CV 3/5; 4/8] START learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 3/5; 4/8] END learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-1.370 total time=  29.9s\n",
            "[CV 4/5; 4/8] START learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 4/5; 4/8] END learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-1.369 total time=  29.6s\n",
            "[CV 5/5; 4/8] START learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 5/5; 4/8] END learning_rate=0.1, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-1.372 total time=  29.9s\n",
            "[CV 1/5; 5/8] START learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 1/5; 5/8] END learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-3.843 total time=  14.0s\n",
            "[CV 2/5; 5/8] START learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 2/5; 5/8] END learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-3.841 total time=  14.0s\n",
            "[CV 3/5; 5/8] START learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 3/5; 5/8] END learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-3.840 total time=  13.5s\n",
            "[CV 4/5; 5/8] START learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 4/5; 5/8] END learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-3.838 total time=  14.2s\n",
            "[CV 5/5; 5/8] START learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror\n",
            "[CV 5/5; 5/8] END learning_rate=0.01, max_depth=3, n_estimators=100, objective=reg:squarederror;, score=-3.840 total time=  17.0s\n",
            "[CV 1/5; 6/8] START learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 1/5; 6/8] END learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-2.680 total time=  24.2s\n",
            "[CV 2/5; 6/8] START learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 2/5; 6/8] END learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-2.680 total time=  21.7s\n",
            "[CV 3/5; 6/8] START learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 3/5; 6/8] END learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-2.679 total time=  25.8s\n",
            "[CV 4/5; 6/8] START learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 4/5; 6/8] END learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-2.677 total time=  22.4s\n",
            "[CV 5/5; 6/8] START learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror\n",
            "[CV 5/5; 6/8] END learning_rate=0.01, max_depth=3, n_estimators=200, objective=reg:squarederror;, score=-2.678 total time=  25.8s\n",
            "[CV 1/5; 7/8] START learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 1/5; 7/8] END learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-3.406 total time=  17.0s\n",
            "[CV 2/5; 7/8] START learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 2/5; 7/8] END learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-3.404 total time=  16.9s\n",
            "[CV 3/5; 7/8] START learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 3/5; 7/8] END learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-3.403 total time=  16.8s\n",
            "[CV 4/5; 7/8] START learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 4/5; 7/8] END learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-3.401 total time=  16.1s\n",
            "[CV 5/5; 7/8] START learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror\n",
            "[CV 5/5; 7/8] END learning_rate=0.01, max_depth=5, n_estimators=100, objective=reg:squarederror;, score=-3.402 total time=  17.6s\n",
            "[CV 1/5; 8/8] START learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 1/5; 8/8] END learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-2.127 total time=  33.4s\n",
            "[CV 2/5; 8/8] START learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 2/5; 8/8] END learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-2.126 total time=  32.6s\n",
            "[CV 3/5; 8/8] START learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 3/5; 8/8] END learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-2.124 total time=  32.1s\n",
            "[CV 4/5; 8/8] START learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 4/5; 8/8] END learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-2.123 total time=  32.1s\n",
            "[CV 5/5; 8/8] START learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror\n",
            "[CV 5/5; 8/8] END learning_rate=0.01, max_depth=5, n_estimators=200, objective=reg:squarederror;, score=-2.122 total time=  33.3s\n",
            "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'objective': 'reg:squarederror'}\n",
            "XGBoost after Grid Search Validation RMSE: 2.0326543819563065\n",
            "XGBoost after Grid Search Validation MAE: 1.3722901025951728\n",
            "XGBoost after Grid Search Test RMSE: 2.03559697720603\n",
            "XGBoost after Grid Search Test MAE: 1.3726895795256981\n",
            "building tree 1 of 100\n",
            "building tree 2 of 100\n",
            "building tree 3 of 100\n",
            "building tree 4 of 100\n",
            "building tree 5 of 100\n",
            "building tree 6 of 100\n",
            "building tree 7 of 100\n",
            "building tree 8 of 100\n",
            "building tree 9 of 100\n",
            "building tree 10 of 100\n",
            "building tree 11 of 100\n",
            "building tree 12 of 100\n",
            "building tree 13 of 100\n",
            "building tree 14 of 100\n",
            "building tree 15 of 100\n",
            "building tree 16 of 100\n",
            "building tree 17 of 100\n",
            "building tree 18 of 100\n",
            "building tree 19 of 100\n",
            "building tree 20 of 100\n",
            "building tree 21 of 100\n",
            "building tree 22 of 100\n",
            "building tree 23 of 100\n",
            "building tree 24 of 100\n",
            "building tree 25 of 100\n",
            "building tree 26 of 100\n",
            "building tree 27 of 100\n",
            "building tree 28 of 100\n",
            "building tree 29 of 100\n",
            "building tree 30 of 100\n",
            "building tree 31 of 100\n",
            "building tree 32 of 100\n",
            "building tree 33 of 100\n",
            "building tree 34 of 100\n",
            "building tree 35 of 100\n",
            "building tree 36 of 100\n",
            "building tree 37 of 100\n",
            "building tree 38 of 100\n",
            "building tree 39 of 100\n",
            "building tree 40 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed: 76.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 41 of 100\n",
            "building tree 42 of 100\n",
            "building tree 43 of 100\n",
            "building tree 44 of 100\n",
            "building tree 45 of 100\n",
            "building tree 46 of 100\n",
            "building tree 47 of 100\n",
            "building tree 48 of 100\n",
            "building tree 49 of 100\n",
            "building tree 50 of 100\n",
            "building tree 51 of 100\n",
            "building tree 52 of 100\n",
            "building tree 53 of 100\n",
            "building tree 54 of 100\n",
            "building tree 55 of 100\n",
            "building tree 56 of 100\n",
            "building tree 57 of 100\n",
            "building tree 58 of 100\n",
            "building tree 59 of 100\n",
            "building tree 60 of 100\n",
            "building tree 61 of 100\n",
            "building tree 62 of 100\n",
            "building tree 63 of 100\n",
            "building tree 64 of 100\n",
            "building tree 65 of 100\n",
            "building tree 66 of 100\n",
            "building tree 67 of 100\n",
            "building tree 68 of 100\n",
            "building tree 69 of 100\n",
            "building tree 70 of 100\n",
            "building tree 71 of 100\n",
            "building tree 72 of 100\n",
            "building tree 73 of 100\n",
            "building tree 74 of 100\n",
            "building tree 75 of 100\n",
            "building tree 76 of 100\n",
            "building tree 77 of 100\n",
            "building tree 78 of 100\n",
            "building tree 79 of 100\n",
            "building tree 80 of 100\n",
            "building tree 81 of 100\n",
            "building tree 82 of 100\n",
            "building tree 83 of 100\n",
            "building tree 84 of 100\n",
            "building tree 85 of 100\n",
            "building tree 86 of 100\n",
            "building tree 87 of 100\n",
            "building tree 88 of 100\n",
            "building tree 89 of 100\n",
            "building tree 90 of 100\n",
            "building tree 91 of 100\n",
            "building tree 92 of 100\n",
            "building tree 93 of 100\n",
            "building tree 94 of 100\n",
            "building tree 95 of 100\n",
            "building tree 96 of 100\n",
            "building tree 97 of 100\n",
            "building tree 98 of 100\n",
            "building tree 99 of 100\n",
            "building tree 100 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Validation RMSE: 0.7861490352259343\n",
            "Random Forest Validation MAE: 0.24096542849227068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Test RMSE: 0.7893800957489981\n",
            "Random Forest Test MAE: 0.2420987765775234\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}